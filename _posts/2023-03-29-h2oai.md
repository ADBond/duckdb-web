---
layout: post
title:  "The Return of the H2oai Benchmark"
author: Tom Ebergen
excerpt_separator: <!--more-->
---

*TL;DR: We re-ran the h2oai db-benchmarks with up-to-date solutions and DuckDB is consistently one of the top performers.*


[Skip directly to the results](#results)

The H2O.ai [DB benchmark](https://h2oai.github.io/db-benchmark/) is a well-known benchmark in the data analytics and R community. The benchmark measures the groupby and join performance of various analytical tools like data.table, polars, dplyr, clickhouse, duckdb and more. Since July 2nd 2021, the benchmark has been dormant, with no result updates or maintenance. Many of the analytical systems measured in the benchmark have since undergone substantial improvements, leaving many of the maintainers curious as to where their analytical tool ranks on the benchmark.

DuckDB has decided to give the H2O.ai benchmark new life and maintain it for the foreseeable future. One reason for DuckDB labs' decision to maintain the benchmark is that duckdb has had 3 new minor releases since the most recent published results on July 2nd, 2021. After managing to run parts of the benchmark on a r3-8xlarge AWS box, DuckDB ranked as a top performer on the benchmark. Therefore, the decision was made to fork the benchmark, modernize the underlying dependencies and run the benchmark on the latest versions of the included systems. You can find the repository [here](https://github.com/duckdblabs/h2oai-db-benchmark).

The results of the new benchmark are very interesting, but first a quick summary of the benchmark and what updates took place.

## The H2O AI benchmark
There are 5 basic grouping tests and 5 advanced grouping tests. The 10 grouping queries all focus on a combination of the following
- Low cardinality (a few big groups)
- High cardinality (lots of very small groups)
- Grouping integer types
- Grouping string types

Each query is run only twice with both results being reported. This way we can see the performance of a cold run and any effects data caching may have. The idea is to avoid reporting any potential "best" results on a hot system. Data analysts only need to run a query once to get their answer. No one drives to the store a second time to get another litre of milk faster.

The time reported is the sum of the time it takes to run all 5 queries twice.

More information about the specific queries can be found below.

#### The Data and Queries

The queries have not changed since the benchmark went dormant. The data is generated in a rather simple manner. Inspecting the datagen files you can see that the columns are generated with small, medium, and large groups of char and int values. Similar generation logic applies to the join data generation.

| Query | SQL |  Objective |
|-----------|---------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| groupby #1  |   `SELECT id1, sum(v1) AS v1 FROM tbl GROUP BY id1` |  Many large groups (varchar) |
| groupby #2  |   `SELECT id1, id2, sum(v1) AS v1 FROM tbl GROUP BY id1, id2` | Many small groups (varchar) |
| groupby #3  |   `SELECT id3, sum(v1) AS v1, mean(v3) AS v3 FROM tbl GROUP BY id3`                 |      Sum and mean aggregation function for large groups                                                       |
| groupby #4  |   `SELECT id4, mean(v1) AS v1, mean(v2) AS v2, mean(v3) AS v3 FROM tbl GROUP BY id4`|      Many Large groups (varchar), and many aggregation functions over small & large groups                       |
| groupby #5  |   `SELECT id6, sum(v1) AS v1, sum(v2) AS v2, sum(v3) AS v3 FROM tbl GROUP BY id6`   |      Many small groups (int),                                                                                 |
| groupby #6  |   `SELECT id4, id5, quantile_cont(v3, 0.5) AS median_v3, stddev(v3) AS sd_v3 FROM tbl GROUP BY id4, id5`  | Group by large (id4) and small (id5) groups (both int)                   |
| groupby #7  |   `SELECT id3, max(v1)-min(v2) AS range_v1_v2 FROM tbl GROUP BY id3`                |      Large groups (varchar) with range selections                                                                |
| groupby #8  |   `SELECT id6, v3 AS largest2_v3 FROM (SELECT id6, v3, row_number() OVER (PARTITION BY id6 ORDER BY v3 DESC) AS order_v3 FROM x WHERE v3 IS NOT NULL) sub_query WHERE order_v3 <= 2`                |    Advanced Groupby query              |
| groupby #9  |   `SELECT id2, id4, pow(corr(v1, v2), 2) AS r2 FROM tbl GROUP BY id2, id4`          |      Group by small (varchar) and large (int) groups.                                                            |
| groupby #10 |   `SELECT id1, id2, id3, id4, id5, id6, sum(v3) AS v3, count(*) AS count FROM tbl GROUP BY id1, id2, id3, id4, id5, id`  | Many many small groups                                                   |
| join #1 |`SELECT x.*, small.id4 AS small_id4, v2 FROM x JOIN small USING (id1)`                                                                           |   join on a small table                                                                                                                                  |
| join #2 |`SELECT x.*, medium.id1 AS medium_id1, medium.id4 AS medium_id4, medium.id5 AS medium_id5, v2 FROM x JOIN medium USING (id2)`                    |   inner join on a medium table                                                                                                           |
| join #3 |`SELECT x.*, medium.id1 AS medium_id1, medium.id4 AS medium_id4, medium.id5 AS medium_id5, v2 FROM x LEFT JOIN medium USING (id2)`               |   left join on a medium table                                                                                                          |
| join #4 |`SELECT x.*, medium.id1 AS medium_id1, medium.id2 AS medium_id2, medium.id4 AS medium_id4, v2 FROM x JOIN medium USING (id5)`                    |   join on a medium table using int                                                                                                      |
| join #5 |`SELECT x.*, big.id1 AS big_id1, big.id2 AS big_id2, big.id4 AS big_id4, big.id5 AS big_id5, big.id6 AS big_id6, v2 FROM x JOIN big USING (id3)` |   join on a big table                                                                                                |


### Modifications to the Benchmark & Hardware
No modifications have been made to the queries or the data generation. Some scripts required minor modifications so that the current version of the library could be run. The hardware used is slightly different as the exact AWS offering the benchmark previously used is no longer available. Base libraries have been updated as well. GPU libraries were not tested. 


AWS is a [m4.10xlarge](https://aws.amazon.com/ec2/instance-types/)

- CPU model: Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz 
- CPU cores: 40 
- RAM model: Unknown 
- Memory: 160GB 
- NO GPU specifications 
- R upgraded, 4.0.0 -> 4.2.2 
- Python upgraded 3.\[6\|7\] -> 3.10 

### Changes made to install scripts of other systems
Pandas, Polars, Dask, and Clickhouse required changes to their setup/install scripts. The changes were relatively minor consisting mostly of syntax updates and data ingestion updates. Data ingestion did not affect the reporting timing results.


## RESULTS


<iframe src="https://tmonster.github.io/h2oai-db-benchmark/"  title="h2o db benchmmark" height=500px width=800px></iframe>

You can also look at the results [here](https://tmonster.github.io/h2oai-db-benchmark/). Hopefully you immediately notice that DuckDB ranks first in all of the queries (I mean, why else would we be writing a blog post about it ðŸ˜œ?). A major contributor to our increased performance is [parallel grouped aggregation](https://duckdb.org/2022/03/07/aggregate-hashtable.html) which was merged in March 2022. In addition, DuckDB now supports [enum types](https://duckdb.org/2021/11/26/duck-enum.html), which makes DuckDB `group by` aggregation even faster. [Improvements to the out-of-core hash join](https://github.com/duckdb/duckdb/pull/4970) were merged as well, further improving the performance of our joins.


## Maintenance plan / Questions?

There are many areas in the code where certain tests are automatically nullified. If you believe that is the case for a test case for your system or if you have any other questions, feel free to file an issue here (<- issue tracker for duckdb h2oai benchmark.)

Do you have any other questions? Would you like to have your system added to the benchmark? Please feel free to read the ReadMe.md in the [repository](https://github.com/duckdblabs/h2oai-db-benchmark), and if you still have questions, you can reach out to me at tom@duckdblabs.com or on our [Discord](https://discord.com/invite/tcvwpjfnZx)!
