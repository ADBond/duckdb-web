---
layout: post
title:  "Explore lakeFS Using SQL In-Browser with DuckDB-Wasm"
author: Guest post by Adi Polak
---

We‚Äôre thrilled to share another outstanding feature hot off the press at lakeFS! üéâ Our team introduced experimental support for running SQL queries directly on objects in the lakeFS UI. 

There‚Äôs no need to install or configure anything. Yes, it‚Äôs that easy. ü•≥

## TLDR

[lakeFS](https://lakefs.io/) users can now easily run SQL queries on Parquet and other tabular formats. You can use this new feature to explore data, look at its schema, compare versions, and much more ‚Äì right from your browser.  No need to install any extra tools or worry about additional components, thanks to [DuckDB-Wasm](https://github.com/duckdb/duckdb-wasm)!

<img src="/images/blog/lakeFS/lakeFS_DuckDB_Wasm_demo.gif" alt="lakeFS DuckDB WASM Demo" width=680/>

## Exploring data in object stores is challenging

Running a modern data lake comes with many benefits. You get [cheaper cloud storage](https://aws.amazon.com/blogs/storage/s3-storage-class-price-reductions/) and enjoy [greater scalability](https://aws.amazon.com/blogs/aws/amazon-s3-bigger-and-busier-than-ever/). Many compute frameworks can access this type of centrally managed storage system.

But here‚Äôs the other side of the coin. 

**Object stores - which are the bedrock of every cloud data lake - weren‚Äôt actually designed to serve as data warehouses.**

## We solved this issue; here‚Äôs how

We set out to change that using the skills of our lakeFS team. 

The lakeFS UI already lets users easily explore data at logical and consistent versions. They can also attach meaningful branch names and commit messages. lakeFS even renders Markdown files so documentation can co-exist with data.

But until now, to actually interact with data, users needed to use an external tool: Spark, Trino, Athena, Azure Synapse, or something else. Such a tool was key for making sense of the data files stored within a specific repository or commit.

## Enter DuckDB

[DuckDB](https://duckdb.org/) is an open-source in-process SQL OLAP database developed in C++. It lets you run very fast (vectorized) SQL queries on data files such as CSVs and [Apache Parquet](https://parquet.apache.org/).

What exactly is a vectorized SQL query? üëÄ

In vectorized query execution, columns are split into batches of rows where each batch can be processed in a single operation. These batches are sized to fit nicely in the fastest CPU caches to keep the processor fully utilised and not slowed down by re-reading data from RAM. It also allows modern CPUs to use vector instructions to process multiple calculations at once. üí° This slashes the CPU cycles required for analytical queries like scanning, aggregating, or joining large datasets for significant speed gains. 
 
Now that you know what a vectorized SQL query is, let‚Äôs return to our DuckDB thread. üß∂

More recently, DuckDB‚Äôs popularity has been rising. No wonder - data engineers are always on the lookout for simple and high-performing ways to explore data without being forced to run expensive and complex distributed systems.

<img src="/images/blog/lakeFS/DuckDB_star_history_2022_03_01.png" alt="DuckDB Github Star History." width=680/>
<p align="center">DuckDB star history graph. Source: https://star-history.com/</p>

An interesting aspect of this project is that it also targets [WebAssembly (Wasm)](https://webassembly.org/), which is a relatively new standard that lets engineers compile code in C++, Rust, and many other languages into a portable binary instruction format that can run directly in your Web browser. Amazing, right? ü§Ø

## Running DuckDB within the lakeFS UI

lakeFS now includes experimental support for running SQL queries directly on objects. 

To use the new feature, click on a supported file in the lakeFS UI. The currently supported formats are CSV, TSV, and Parquet.

You‚Äôll get a SQL shell. A default query will select a sample of rows from the chosen file.

<img src="/images/blog/lakeFS/lakeFS_DuckDB_Wasm_example.jpg" alt="lakeFS DuckDB Wasm Example" width=680/>

You can use the [full breadth of supported SQL functions](https://duckdb.org/docs/sql/introduction#querying-a-table) - yes, even JOINs! - from the comfort of your web browser of choice. 

You can use this feature to describe the contents of a Parquet file to understand its schema, compare objects between different versions to see how the data changed by leveraging `lakefs_object` functionality with different branches ( aka. versions) or pick a small sample to gain a quick grasp of an unfamiliar dataset. 

The possibilities are endless! 

## ‚úÖ Get started

To query data in the lakeFS UI, ensure that your current [lakeFS installation](https://github.com/treeverse/lakeFS) is on version 0.88.0 or higher. No additional configuration is required at this point.

If you‚Äôre not a proud lakeFS user yet, you can [quickly set up a local lakeFS instance](https://docs.lakefs.io/quickstart/run.html) and import your existing data into it. 

*Note: Importing data into lakeFS doesn‚Äôt copy or modify anything, it‚Äôs a metadata operation where lakeFS creates pointers to your existing data objects.*

Use this one-liner to run lakeFS with the S3 storage adapter. It will let you import data from your existing object store:

```
docker run --pull always -p 8000:8000 \
   -e LAKEFS_BLOCKSTORE_TYPE='s3' \
   -e AWS_ACCESS_KEY_ID='YourAccessKeyValue' \
   -e AWS_SECRET_ACCESS_KEY='YourSecretKeyValue' \
   treeverse/lakefs run --local-settings
```

For instructions on how to run lakeFS with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Google Cloud Storage](https://cloud.google.com/storage), or [MinIO](https://min.io/), check out the [lakeFS quick start guide](https://docs.lakefs.io/quickstart/run.html#running-locally-with-docker-connected-to-an-object-store).

Once you‚Äôre up there, use the [zero-copy import tool](https://docs.lakefs.io/howto/import.html#zero-copy-import) to get your data showing in lakeFS.

## ‚ù§Ô∏è Please share your feedback with our team to help lakeFS better support DuckDB. ‚ù§Ô∏è

You can learn more about the integration in the [lakeFS blog](https://lakefs.io/blog/lakefs-duckdb-embedding-an-olap-database-in-the-lakefs-ui/), and don‚Äôt forget to join the awesome [lakeFS data practitioners‚Äô Slack community](https://join.slack.com/t/lakefs/shared_invite/zt-1kokn8lv9-xL1Gp~V6JLx7bEhTpobzvg), and the [DuckDB Discord Wasm channel](https://discord.gg/7JstQKxFJq) to meet like-minded people, ask questions, and share your wicked-smart insights! 
